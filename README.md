# Plano TÃ©cnico Detalhado: Benchmark de Bibliotecas HTTP

Vou propor uma **reestruturaÃ§Ã£o completa** do seu projeto, mantendo o foco em **bibliotecas HTTP** (nÃ£o frameworks), com metodologia rigorosa e resultados reprodutÃ­veis.

---

## ðŸ“‹ Arquitetura Proposta

### Estrutura de DiretÃ³rios
```
benchmark-http-libs/
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ server.go
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ go.mod
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ client_requests.py
â”‚   â”‚   â”œâ”€â”€ client_httpx.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ javascript/
â”‚   â”‚   â”œâ”€â”€ client_axios.js
â”‚   â”‚   â”œâ”€â”€ client_undici.js
â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ go/
â”‚   â”‚   â”œâ”€â”€ client_nethttp.go
â”‚   â”‚   â”œâ”€â”€ client_fasthttp.go
â”‚   â”‚   â”œâ”€â”€ go.mod
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ elixir/
â”‚       â”œâ”€â”€ client_httpoison.exs
â”‚       â”œâ”€â”€ client_finch.exs
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ run_benchmark.py
â”‚   â”œâ”€â”€ metrics_collector.py
â”‚   â”œâ”€â”€ report_generator.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ results/
â”‚   â””â”€â”€ (gerado automaticamente)
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ðŸŽ¯ Metodologia de Teste

### Fase 1: Warm-up (2 minutos)
- EstabilizaÃ§Ã£o do servidor
- Aquecimento de caches
- **NÃ£o coleta mÃ©tricas**

### Fase 2: ExecuÃ§Ã£o (3 minutos)
- Coleta de mÃ©tricas detalhadas
- MediÃ§Ã£o de latÃªncia, throughput e erros

### NÃ­veis de ConcorrÃªncia
Cada biblioteca serÃ¡ testada com: **8, 32, 128 e 512** conexÃµes simultÃ¢neas

### MÃ©tricas Coletadas
- **Throughput**: requisiÃ§Ãµes/segundo
- **LatÃªncia**: mÃ©dia, p50, p95, p99
- **Taxa de erros**: % de requisiÃ§Ãµes falhadas
- **Recursos**: CPU e memÃ³ria (mÃ©dia e pico)

---

## ðŸ“Š Como Executar

### 1. Teste Individual (uma biblioteca, uma concorrÃªncia)

```bash
# Exemplo: testar requests com concorrÃªncia 8
docker-compose up server client_requests_8
```

### 2. ExecuÃ§Ã£o Completa Automatizada

```bash
# Instalar dependÃªncias do orchestrator
cd orchestrator
pip install -r requirements.txt

# Executar todos os testes
python run_benchmark.py

# Gerar relatÃ³rios
python report_generator.py
```

### 3. Estrutura de Resultados Gerados

```
results/
â”œâ”€â”€ requests_c8.json
â”œâ”€â”€ requests_c32.json
â”œâ”€â”€ httpx_c8.json
â”œâ”€â”€ undici_c8.json
â”œâ”€â”€ ...
â”œâ”€â”€ summary.json
â”œâ”€â”€ comparison_table.md
â”œâ”€â”€ benchmark_results.csv
â”œâ”€â”€ chart_throughput.png
â”œâ”€â”€ chart_latency_p95.png
â”œâ”€â”€ chart_error_rate.png
â””â”€â”€ chart_latency_distribution.png
```

---

## ðŸŽ¯ Principais Melhorias Implementadas

### âœ… ConcorrÃªncia Real
- Cada cliente implementa concorrÃªncia apropriada para sua linguagem
- Python: ThreadPoolExecutor (requests) e asyncio (httpx)
- JavaScript: Promise.all com workers
- Go: Goroutines
- Elixir: Processos Erlang

### âœ… Metodologia Rigorosa
- Warm-up de 2 minutos (configurÃ¡vel)
- ExecuÃ§Ã£o de 3 minutos (configurÃ¡vel)
- 4 nÃ­veis de concorrÃªncia: 8, 32, 128, 512

### âœ… MÃ©tricas Completas
- Throughput (req/s)
- LatÃªncia: mÃ©dia, p50, p95, p99, min, max
- Taxa de erros (%)
- Total de requisiÃ§Ãµes bem-sucedidas/falhadas

### âœ… Recursos Balanceados
- Limites de CPU e memÃ³ria via Docker deploy
- Todos os clientes: 1 CPU, 512MB RAM
- Servidor: 2 CPUs, 2GB RAM
- Isolamento via cgroups do Docker

### âœ… AutomaÃ§Ã£o Completa
- Orquestrador Python executa todos os testes sequencialmente
- GeraÃ§Ã£o automÃ¡tica de relatÃ³rios
- ExportaÃ§Ã£o em mÃºltiplos formatos (JSON, CSV, Markdown)
- GrÃ¡ficos comparativos

### âœ… Reprodutibilidade
- ConfiguraÃ§Ã£o via variÃ¡veis de ambiente
- Docker garante ambiente consistente
- Resultados salvos em JSON estruturado
- DocumentaÃ§Ã£o completa

---

## ðŸ“ˆ Exemplo de SaÃ­da

```json
{
  "library": "undici",
  "language": "javascript",
  "concurrency": 8,
  "duration": 180,
  "total_requests": 145823,
  "successful_requests": 145820,
  "failed_requests": 3,
  "error_rate": 0.002,
  "throughput": 810.13,
  "latency_avg_ms": 9.87,
  "latency_p50_ms": 8.45,
  "latency_p95_ms": 15.32,
  "latency_p99_ms": 22.18,
  "latency_min_ms": 2.14,
  "latency_max_ms": 145.67
}
```

---

## ðŸ”§ PersonalizaÃ§Ã£o

### Ajustar DuraÃ§Ã£o dos Testes

Edite as variÃ¡veis de ambiente no `docker-compose.yml`:

```yaml
environment:
  - WARMUP_DURATION=60    # Reduzir para testes rÃ¡pidos
  - TEST_DURATION=300     # Aumentar para maior precisÃ£o
```

### Adicionar Mais NÃ­veis de ConcorrÃªncia

Adicione novos serviÃ§os ao `docker-compose.yml` e atualize `CONCURRENCY_LEVELS` no orchestrator.

### Limitar Recursos

Ajuste os limites em `deploy.resources` conforme necessÃ¡rio.
